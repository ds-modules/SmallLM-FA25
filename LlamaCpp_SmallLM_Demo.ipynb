{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Intro to AI workflows using Small LM from Huggingface\n",
    "## Using llama-cpp-python Package\n",
    "\n",
    "This notebook is adapted from the [GPT4All_SmallLM_Demo.ipynb](GPT4All_SmallLM_Demo.ipynb) notebook and demonstrates how to work with small language models using the **llama-cpp-python** package instead of GPT4All.\n",
    "\n",
    "### What is llama-cpp-python?\n",
    "\n",
    "[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) provides Python bindings for the [llama.cpp](https://github.com/ggml-org/llama.cpp) library, which is a high-performance C++ implementation for running Large Language Models (LLMs) locally. It's one of the most popular libraries for running GGUF models on consumer hardware.\n",
    "\n",
    "### Key Features of llama-cpp-python:\n",
    "\n",
    "1. **Efficient Local Inference**: Runs models entirely on CPU (with optional GPU acceleration)\n",
    "2. **GGUF Model Support**: Works with quantized models in the GGUF format from Hugging Face\n",
    "3. **Low Memory Footprint**: Supports 4-bit, 8-bit, and other quantization levels\n",
    "4. **OpenAI-Compatible API**: Includes a built-in server that mimics OpenAI's API\n",
    "5. **Active Development**: Regular updates to support new model architectures\n",
    "6. **Cross-Platform**: Works on Windows, macOS, and Linux\n",
    "\n",
    "### Why use llama-cpp-python over GPT4All?\n",
    "\n",
    "| Feature | llama-cpp-python | GPT4All |\n",
    "|---------|-----------------|--------|\n",
    "| Model Support | Any GGUF model from Hugging Face | Curated list of ~30 models |\n",
    "| Updates | Very frequent (follows llama.cpp) | Less frequent |\n",
    "| API Style | Direct Python + OpenAI-compatible server | Custom Python API |\n",
    "| Flexibility | More control over inference parameters | Simpler, more abstracted |\n",
    "| Community | Large, active community | Smaller, focused community |\n",
    "\n",
    "**Fun Fact**: GPT4All actually uses llama.cpp as its backend! So this notebook gives you more direct access to the underlying inference engine.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- llama-cpp-python [GitHub Repository](https://github.com/abetlen/llama-cpp-python)\n",
    "- llama-cpp-python [Documentation](https://llama-cpp-python.readthedocs.io/)\n",
    "- llama.cpp [GitHub Repository](https://github.com/ggml-org/llama.cpp)\n",
    "- Hugging Face [GGUF Models](https://huggingface.co/models?search=gguf)\n",
    "\n",
    "### Attribution\n",
    "\n",
    "Notebook originally developed based on work by Greg Merritt <[gmerritt@berkeley.edu](mailto:gmerritt@berkeley.edu)> and adapted by Eric Van Dusen. This llama-cpp-python version was created as an alternative implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "### Installing llama-cpp-python\n",
    "\n",
    "The installation of llama-cpp-python is straightforward but has some important considerations:\n",
    "\n",
    "1. **CPU-only installation** (what we'll use): `pip install llama-cpp-python`\n",
    "2. **GPU acceleration (CUDA)**: Requires building from source with CUDA support\n",
    "3. **Metal acceleration (macOS)**: Automatic on Apple Silicon\n",
    "\n",
    "**Note**: The first time you run inference, the model needs to be loaded into memory. This may take a moment depending on the model size and your hardware.\n",
    "\n",
    "### Steps:\n",
    "1. Ensure that your Python environment has llama-cpp-python installed\n",
    "2. Define the model path where your `.gguf` model files are stored\n",
    "3. Load a model using the `Llama` class\n",
    "\n",
    "_This notebook assumes that at least one 'Small model' file ending in `.gguf` has already been downloaded into a directory (see `GPT4All_Download_gguf.ipynb` for more)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your python environment has llama-cpp-python capability\n",
    "# Note: This uses the installation pattern specified for this notebook\n",
    "try: \n",
    "    from llama_cpp import Llama\n",
    "except: \n",
    "    %pip install llama-cpp-python\n",
    "    from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Llama Class\n",
    "\n",
    "The `Llama` class is the main interface for loading and interacting with GGUF models. Key parameters include:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `model_path` | Full path to the .gguf model file | Required |\n",
    "| `n_ctx` | Context window size (max tokens model can see) | 512 |\n",
    "| `n_threads` | Number of CPU threads to use | Auto |\n",
    "| `n_gpu_layers` | Layers to offload to GPU (0 = CPU only) | 0 |\n",
    "| `verbose` | Print loading information | True |\n",
    "| `chat_format` | Chat template format (e.g., \"chatml\", \"llama-2\") | Auto-detect |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out our local filesystem path and whether we have files downloaded\n",
    "\n",
    "We need to locate where our `.gguf` model files are stored. Below are examples for different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - if a Shared Hub is being used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only worked for FA 25 workshop on Cal ICOR Hub\n",
    "#!ls /home/jovyan/shared_readwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Cal-ICOR workshop hub (JupyterCon Nov 2025)\n",
    "!ls /home/jovyan/shared/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - if a local machine is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is my local path to a directory called shared-rw\n",
    "!ls shared-rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the full path ( this is on my laptop) \n",
    "!ls /Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pick your environment - Local vs Hub - and set the Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model path parameter depending on where you are computing\n",
    "model_directory = \"/home/jovyan/shared/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model path parameter depending on where you are computing\n",
    "#model_directory = \"/Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading the Downloaded Model with llama-cpp-python\n",
    "\n",
    "In this step, we create a local instance of the model using the `Llama` class.  \n",
    "\n",
    "**Key differences from GPT4All:**\n",
    "- We provide the **full path** to the model file (not just the filename)\n",
    "- We can specify `n_ctx` (context window size) directly\n",
    "- We have fine-grained control over threading with `n_threads`\n",
    "- The `chat_format` parameter helps the model understand conversation structure\n",
    "\n",
    "**About the model:**\n",
    "`qwen2-1_5b-instruct-q4_0.gguf` is a **1.5 billion-parameter Qwen2 model** that has been **quantized** to reduce its size and memory usage. The `.gguf` extension indicates that the model is stored in the **GGUF format**, which is the standard format for llama.cpp inference.\n",
    "\n",
    "**Note:**  \n",
    "Loading the model may take a few seconds. You'll see verbose output showing the model configuration being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the model filename\n",
    "model_name = \"qwen2-1_5b-instruct-q4_0.gguf\"\n",
    "\n",
    "# Create the full path to the model\n",
    "model_path = os.path.join(model_directory, model_name)\n",
    "\n",
    "# Load the model using llama-cpp-python\n",
    "# n_ctx: context window size (how many tokens the model can \"see\" at once)\n",
    "# n_threads: number of CPU threads (None = auto-detect)\n",
    "# verbose: whether to print loading information\n",
    "# chat_format: the chat template format for this model family\n",
    "model = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,          # Context window size\n",
    "    n_threads=None,      # Auto-detect optimal thread count\n",
    "    verbose=True,        # Print model loading info\n",
    "    chat_format=\"chatml\" # Qwen uses ChatML format\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Call the model with a simple user message\n",
    "\n",
    "### Using create_chat_completion()\n",
    "\n",
    "In llama-cpp-python, we use the `create_chat_completion()` method to generate responses. This method follows the OpenAI Chat Completions API format, making it easy to switch between local models and cloud APIs.\n",
    "\n",
    "**Message Structure:**\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"System instructions here\"},\n",
    "    {\"role\": \"user\", \"content\": \"User message here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Previous assistant response (optional)\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**This may take a few moments to process.**\n",
    "\n",
    "You may run this multiple times, and will likely get different results. Feel free to change the `user_message`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_message = \"Who pays for tariffs on foreign manufactured goods? Consumer or Producer?\"  # You can change this prompt\n",
    "\n",
    "# Create the messages list (OpenAI-compatible format)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "# Generate a response using create_chat_completion\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "The `create_chat_completion()` method returns a dictionary that follows the OpenAI API response format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"id\": \"chatcmpl-...\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": 1234567890,\n",
    "    \"model\": \"model_name\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"The actual response text...\"\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 10,\n",
    "        \"completion_tokens\": 50,\n",
    "        \"total_tokens\": 60\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "To get the text content, we access: `response[\"choices\"][0][\"message\"][\"content\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Passing additional arguments to control generation\n",
    "\n",
    "The `create_chat_completion()` method accepts many parameters to control the generation:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `messages` | List of message dictionaries | Required |\n",
    "| `max_tokens` | Maximum number of tokens to generate | 16 |\n",
    "| `temperature` | Controls randomness (0 = deterministic, higher = more random) | 0.8 |\n",
    "| `top_p` | Nucleus sampling (consider tokens with top_p cumulative probability) | 0.95 |\n",
    "| `top_k` | Only consider the top_k most likely tokens | 40 |\n",
    "| `repeat_penalty` | Penalize repeated tokens (1.0 = no penalty) | 1.1 |\n",
    "| `stream` | If True, returns a generator for streaming responses | False |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Using the `max_tokens` argument to cap the length of the response\n",
    "\n",
    "Generation will stop abruptly once it reaches the maximum number of tokens, even if the response is mid-sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 60  # You can change this parameter\n",
    "\n",
    "user_message = \"What is the economic outcome of tariffs on foreign manufactured goods?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=response_size_limit_in_tokens\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. The `temperature` argument\n",
    "\n",
    "LLMs generate one token (\"word\") at a time as they complete the text. At each step, there's a probability distribution over possible next tokens. The **temperature** parameter controls how this distribution is sampled:\n",
    "\n",
    "- **`temperature = 0`** (\"cold\"): Always picks the most likely token → deterministic output\n",
    "- **`temperature = 0.5-0.7`** (\"warm\"): Balanced creativity and coherence\n",
    "- **`temperature = 1.0`** (\"hot\"): High variety but may be less coherent\n",
    "- **`temperature > 1.0`** (\"very hot\"): Very random, often incoherent\n",
    "\n",
    "**Let's run the same prompt three times with `temperature = 0`; we expect identical outputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 3\n",
    "temperature = 0.0  # You can change this parameter\n",
    "\n",
    "user_message = \"How will tariffs affect the prices of foreign manufactured goods\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=response_size_limit_in_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    print(f\"{response['choices'][0]['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's repeat with a slightly \"hotter\" temperature of `temperature = 0.25`; we expect outputs to begin diverging:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 3\n",
    "temperature = 0.25\n",
    "\n",
    "user_message = \"How will tariffs affect elections\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=response_size_limit_in_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    print(f\"{response['choices'][0]['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A \"very hot\" temperature of `temperature = 1` will result in high variety but may lead to less satisfactory responses:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 5\n",
    "temperature = 1\n",
    "\n",
    "user_message = \"How will tariffs affect elections\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=response_size_limit_in_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    print(f\"{response['choices'][0]['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Include a hidden \"system message\" at the start of the conversation\n",
    "\n",
    "A **system message** sets the context and personality for the assistant. It's placed at the beginning of the messages list and influences how the model responds to all subsequent user messages.\n",
    "\n",
    "In llama-cpp-python, we simply include a message with `\"role\": \"system\"` as the first item in the messages list:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your system instructions here...\"},\n",
    "    {\"role\": \"user\", \"content\": \"User's question\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**Note:** System messages are never guaranteed to remain secret; models can sometimes be prompted to reveal their instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 100\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a hard working economics student at UC Berkeley. \n",
    "You think that there may be some truth to the things you learn in economics classes.\n",
    "You wish that the people in the government understood economics.\n",
    "You think that memes and poems and pop songs are a good way to communicate\n",
    "Answer in rap lyrics always\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"How will tariffs affect inflation\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=response_size_limit_in_tokens\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. \"Few-shot\" learning: include conversation history to set the tone\n",
    "\n",
    "**Few-shot learning** involves providing example prompt/response pairs to establish a pattern. The model will statistically tend to follow this pattern when generating new responses.\n",
    "\n",
    "In llama-cpp-python, this is elegantly handled by simply including multiple `user`/`assistant` message pairs before the actual user query:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"System prompt\"},\n",
    "    {\"role\": \"user\", \"content\": \"Example question 1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Example answer 1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Example question 2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Example answer 2\"},\n",
    "    {\"role\": \"user\", \"content\": \"Actual user question\"}  # Real question\n",
    "]\n",
    "```\n",
    "\n",
    "The model learns the response style from the examples and applies it to the new question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. A \"Few-shot\" example\n",
    "\n",
    "In this example, we establish a pattern of concise, informative responses about economics. The model should follow this established conversational style.\n",
    "\n",
    "**Note:** We're using the native message format which llama-cpp-python automatically converts to the appropriate chat template (ChatML for Qwen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 200\n",
    "\n",
    "# System message sets the overall behavior\n",
    "system_message = \"\"\"\n",
    "You are an economics tutor with a focus on international trade.\n",
    "Answer concisely and clearly, using accessible language.\n",
    "\"\"\"\n",
    "\n",
    "# Few-shot examples establish the response pattern\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # Example 1\n",
    "    {\"role\": \"user\", \"content\": \"What is a tariff?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A tariff is a tax imposed by a government on imported goods, often used to protect domestic industries.\"},\n",
    "    # Example 2\n",
    "    {\"role\": \"user\", \"content\": \"How do tariffs affect consumer prices?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Tariffs typically raise the price of imported goods, making them more expensive for consumers.\"},\n",
    "    # Example 3\n",
    "    {\"role\": \"user\", \"content\": \"Can tariffs backfire?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, they can lead to trade wars, hurt exporters, and reduce overall economic efficiency.\"},\n",
    "    # Example 4\n",
    "    {\"role\": \"user\", \"content\": \"How do other countries respond to tariffs?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"They often retaliate with their own tariffs, targeting key export sectors.\"},\n",
    "    # The actual user question\n",
    "    {\"role\": \"user\", \"content\": \"What is an example of a real-world tariff dispute?\"}\n",
    "]\n",
    "\n",
    "# Generate response\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=response_size_limit_in_tokens,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. The importance of proper chat formatting\n",
    "\n",
    "One advantage of llama-cpp-python is that it **automatically handles chat templating** when you set the `chat_format` parameter (we set it to `\"chatml\"` for Qwen models).\n",
    "\n",
    "Behind the scenes, your messages are converted to special tokens like:\n",
    "```\n",
    "<|im_start|>system\n",
    "You are an economics tutor...\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "What is a tariff?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "If you used the wrong format (or no format), the model might:\n",
    "- Continue writing the script rather than responding as an assistant\n",
    "- Produce incoherent output\n",
    "- Not follow the conversation structure\n",
    "\n",
    "**The `chat_format` parameter ensures proper formatting automatically!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. A note about \"hallucinations\"\n",
    "\n",
    "It's popular to use the word \"hallucinations\" to talk about model output that is very different from what we wanted, or when the output does not seem to make sense.\n",
    "\n",
    "However, an LLM does not perceive; it merely predicts the most likely next token based on patterns in its training data. The term \"hallucination\" can be misleading because:\n",
    "\n",
    "1. **The model isn't failing** — it's doing exactly what it's designed to do\n",
    "2. **It's a statistical process** — sometimes low-probability completions happen\n",
    "3. **Training data limitations** — the model can only draw from what it learned\n",
    "\n",
    "Understanding this helps us have realistic expectations and design better prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Building a chatbot application\n",
    "\n",
    "If you wanted to build an extended conversation experience, you would:\n",
    "\n",
    "1. **Maintain a message history list** that grows with each exchange\n",
    "2. **Append new user messages** to the history\n",
    "3. **Append assistant responses** to the history after generation\n",
    "4. **Pass the entire history** to each new call\n",
    "\n",
    "```python\n",
    "# Example chatbot loop structure\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = model.create_chat_completion(messages=messages)\n",
    "    assistant_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    print(f\"Assistant: {assistant_message}\")\n",
    "```\n",
    "\n",
    "**Important:** The LLM itself has no \"memory\" — it's your application that stores and manages the conversation history. Each call processes the entire conversation from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus: Streaming responses\n",
    "\n",
    "llama-cpp-python supports **streaming**, which lets you see tokens as they're generated (like ChatGPT). This is useful for:\n",
    "- Better user experience (immediate feedback)\n",
    "- Long responses (no waiting for complete generation)\n",
    "- Real-time applications\n",
    "\n",
    "Set `stream=True` to enable streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Explain the concept of comparative advantage in international trade.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "print(\"Response (streaming):\")\n",
    "\n",
    "# With streaming, we get a generator that yields chunks\n",
    "stream = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=150,\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "# Print each chunk as it arrives\n",
    "for chunk in stream:\n",
    "    delta = chunk[\"choices\"][0][\"delta\"]\n",
    "    if \"content\" in delta:\n",
    "        print(delta[\"content\"], end=\"\", flush=True)\n",
    "\n",
    "print()  # New line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Install and import** llama-cpp-python\n",
    "2. **Load a GGUF model** using the `Llama` class\n",
    "3. **Generate responses** using `create_chat_completion()`\n",
    "4. **Control generation** with parameters like `max_tokens`, `temperature`, `top_p`\n",
    "5. **Use system messages** to set assistant behavior\n",
    "6. **Implement few-shot learning** with example conversations\n",
    "7. **Stream responses** for real-time output\n",
    "\n",
    "### Key Advantages of llama-cpp-python:\n",
    "- **OpenAI-compatible API** — easy to switch between local and cloud models\n",
    "- **Any GGUF model** — not limited to a curated list\n",
    "- **Active development** — regular updates for new model architectures\n",
    "- **Fine-grained control** — access to low-level inference parameters\n",
    "\n",
    "### Next Steps:\n",
    "- Try different GGUF models from Hugging Face\n",
    "- Experiment with different temperature and sampling settings\n",
    "- Build a simple chatbot application\n",
    "- Explore GPU acceleration for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
