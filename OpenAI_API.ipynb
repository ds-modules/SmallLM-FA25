{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974a3699-87cd-4c24-b653-bab6230adda6",
   "metadata": {},
   "source": [
    "# Exploring the OpenAI API: Tokens, Costs, and Usage\n",
    "\n",
    "This notebook demonstrates how to interact with the **OpenAI API** from Python in a reproducible classroom or research environment.  \n",
    "The focus is on understanding how **tokenization**, **model usage**, and **costs per token** work in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6aa16e-95df-4ab1-84e4-c56eb8a517f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  What the Notebook Does\n",
    "\n",
    "\n",
    "1. **Connects to the OpenAI API** using the shared API key.  \n",
    "2. **Sends example prompts** to small and large models (e.g., `gpt-4-turbo` or `gpt-4o-mini`) to illustrate response quality and cost trade-offs.  \n",
    "3. **Explores tokenization** — how text is converted into tokens and how token counts vary by model.  \n",
    "4. **Calculates API usage costs**, showing how prompt length and model choice affect pricing.  \n",
    "5. **Visualizes results**, helping students understand the relationship between:\n",
    "   - Input text length (number of tokens)\n",
    "   - Model type and context window\n",
    "   - Cost per request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1784fb-3795-42e4-bc78-622fcfbd20c7",
   "metadata": {},
   "source": [
    "##  Learning Goals\n",
    "\n",
    "- Understand what a **token** is and how it differs from characters or words.  \n",
    "- Learn to estimate and monitor **API usage costs**.  \n",
    "- Gain experience working with **environment variables** and best practices for secret management.  \n",
    "- Build intuition for **trade-offs between model size, latency, and price** in practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3118db0d-0bc8-4419-8dbd-8963a803dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6328f7c2-0df4-4d22-8deb-a03e0ce5567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except:\n",
    "    !pip install python-dotenv\n",
    "    from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a731ec-e558-421a-8ae5-57b81cee984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    !pip install openai\n",
    "    from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f3d98-60b7-4b96-b43d-d3f1f74492d7",
   "metadata": {},
   "source": [
    "\n",
    "##  API Key Setup\n",
    "\n",
    "To keep credentials secure, the API key is **not stored directly in this notebook**.  \n",
    "\n",
    "*The API key is linked  to my credit card, so if it gets out the charges could add up.  If I put the API key on Github it will be automatically flagged.* \n",
    "\n",
    "Instead, it is stored in a `.env` file inside a shared directory (`../shared/.env`) with a line like: `openai_API_KEY=\"...\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1299a86-ad63-4326-b70d-ac04d6de2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../shared/.env')\n",
    "\n",
    "openai_api_key = os.getenv('openai_API_KEY')\n",
    "print(\"API Key loaded:\", \"✅\" if openai_api_key else \"❌ not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821fcea-66bd-4849-8102-a20bf76e3b08",
   "metadata": {},
   "source": [
    "( option to manually load ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613fee75-6964-45a3-a75b-d9c154fad9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is just if you want to manually load a different API Key \n",
    "#openai_API_KEY =\"  \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc48c7-595f-41e3-b565-49cb873ec662",
   "metadata": {},
   "source": [
    "### Notes for Instructors\n",
    "\n",
    "- The shared `.env` file allows multiple users on the same DataHub instance or Jupyter environment to access a single institutional API key without embedding secrets in their notebooks.  \n",
    "- Students should **never print the API key** or share the `.env` file contents publicly.  \n",
    "- The key can be rotated by updating the shared `.env` file; all dependent notebooks will continue to function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f10c0-5575-487a-9920-5f27d8347837",
   "metadata": {},
   "source": [
    "\n",
    "##  The OpenAI Python Package\n",
    "\n",
    "The **OpenAI Python package** provides a simple interface for interacting with OpenAI’s models—such as GPT, Whisper, and DALL·E—directly from Python code. It supports both synchronous and asynchronous API calls, making it easy to send prompts, generate completions, and analyze responses. The package handles authentication via an environment variable (`openai_API_KEY`) and returns structured results that can be easily integrated into data workflows, Jupyter notebooks, or applications for natural language processing, code generation, or AI-assisted analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e013c-f6ea-448e-9611-f71587e0584d",
   "metadata": {},
   "source": [
    "### Initializing the OpenAI Client\n",
    "\n",
    "Once the API key is loaded from the environment, we create a client object that serves as our connection to the OpenAI API.  \n",
    "This client will handle authentication and allow us to make requests to different models.  \n",
    "\n",
    "We’ll initialize it like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5434c-dcb7-4274-99b9-8ef0a42c557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c3afe-cf5b-457a-a200-207ba9bbeae6",
   "metadata": {},
   "source": [
    "### Checking Available Models\n",
    "\n",
    "Before making any API calls, it’s useful to list the models that your API key can access.  \n",
    "The `client.models.list()` method returns all available model identifiers for your OpenAI account, such as `gpt-4o`, `gpt-4-turbo`, and smaller variants like `gpt-4o-mini`.  \n",
    "Listing these helps confirm the correct model names to use in later API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def128a-ea9c-49f4-aa0b-35118ee40065",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list()\n",
    "print([m.id for m in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fb7f6-fc48-462c-b9b6-6f648bd1dc67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Send a chat message to GPT-3.5\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a UC Berkeley Economics Student\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain who pays the burden of tariffs\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b0b9f-5bac-4124-9a0b-35171c3ff475",
   "metadata": {},
   "source": [
    "## Basic Chat Completion Example\n",
    "\n",
    "To demonstrate the simplest API call, we can send a chat-style request to one of the OpenAI language models.  \n",
    "Here, we use `client.chat.completions.create()` to send a short conversation.  \n",
    "The model responds based on the system and user messages provided.\n",
    "\n",
    "In this example, the system message defines the context (“You are a UC Berkeley Economics student”), and the user asks a question (“Explain who pays the burden of tariffs”).  \n",
    "The model returns a text completion that we can extract and display from the `response` object.\n",
    "\n",
    "This basic pattern—system message, user message, and model reply—is the foundation of all chat-based interactions with OpenAI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e8c57-e154-4c86-9f6a-9b1cbd75b394",
   "metadata": {},
   "source": [
    "###  OpenAI Token Pricing (as of April 2025)\n",
    "\n",
    "| Model              | Input Tokens (per 1K) | Output Tokens (per 1K) | Context Window     |\n",
    "|-------------------|-----------------------|-------------------------|--------------------|\n",
    "| **GPT-4 Turbo**    | $0.01                 | $0.03                   | 128K tokens        |\n",
    "| **GPT-4 (legacy)** | $0.03                 | $0.06                   | 8K or 32K tokens   |\n",
    "| **GPT-3.5 Turbo**  | $0.001                | $0.002                  | 16K tokens         |\n",
    "| *GPT-4o* (expected) | *TBD*                | *TBD*                   | *128K tokens*      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03196886-9670-4fde-9275-6913ba9929c9",
   "metadata": {},
   "source": [
    "##  A widget to calculate costs of token consumption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd53752-bee4-4322-b70f-b0719675a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token prices (per 1K tokens)\n",
    "token_prices = {\n",
    "    \"gpt-4-turbo\": {\"input\": 0.01, \"output\": 0.03},\n",
    "    \"gpt-4 (legacy)\": {\"input\": 0.03, \"output\": 0.06},\n",
    "    \"gpt-3.5-turbo\": {\"input\": 0.001, \"output\": 0.002},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f6e5c-2b7c-4943-a6ef-8e84e5591548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=list(token_prices.keys()),\n",
    "    value=\"gpt-3.5-turbo\",\n",
    "    description='Model:',)\n",
    "\n",
    "input_tokens = widgets.IntText(\n",
    "    value=1000,\n",
    "    description='Input Tokens:',)\n",
    "\n",
    "output_tokens = widgets.IntText(\n",
    "    value=500,\n",
    "    description='Output Tokens:',)\n",
    "\n",
    "estimate_button = widgets.Button(\n",
    "    description=\"Estimate Cost\",\n",
    "    button_style=\"success\")\n",
    "\n",
    "cost_display = widgets.Label(value=\"\")\n",
    "\n",
    "# Define the estimator\n",
    "def estimate_cost(b):\n",
    "    model = model_selector.value\n",
    "    input_count = input_tokens.value\n",
    "    output_count = output_tokens.value\n",
    "    prices = token_prices[model] \n",
    "    cost = (input_count / 1000) * prices[\"input\"] + (output_count / 1000) * prices[\"output\"]\n",
    "    cost_display.value = f\"💲 Estimated Cost: ${cost:.6f}\"\n",
    "\n",
    "estimate_button.on_click(estimate_cost)\n",
    "\n",
    "# Display everything\n",
    "display(model_selector, input_tokens, output_tokens, estimate_button, cost_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56769326-5e7a-4d92-9edc-a35f65e4fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a chat message to GPT-3.5\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a UC Berkeley Economics Student\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain who pays the burden of tariffs\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Display response\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Display token usage\n",
    "print(\"\\n🔢 Token Usage:\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763fc99-1359-4ecb-af54-2c4aacad68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a UC Berkeley Economics Student\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain who pays the burden of tariffs\"}\n",
    "    ],\n",
    "    temperature=0.7,               # creativity level (0 = deterministic, 1 = max randomness)\n",
    "    top_p=1.0,                     # nucleus sampling (used instead of temperature, but can be combined)\n",
    "    presence_penalty=0.5,         # encourages new topics\n",
    "    frequency_penalty=0.3,        # discourages repetition\n",
    "    max_tokens=200,               # max length of the response\n",
    "    stop=None                     # can be a list of strings to stop generation early (e.g., [\"\\n\", \"END\"])\n",
    ")\n",
    "\n",
    "# Display the response text\n",
    "print(\"📘 Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Display token usage\n",
    "print(\"\\n🔢 Token Usage:\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
