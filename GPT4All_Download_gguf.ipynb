{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f8bed4",
   "metadata": {},
   "source": [
    "# GPT4All - setup by downloading models\n",
    "\n",
    "## !! This notebook is not meant to be run, just to explain the setup !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d53b1d",
   "metadata": {},
   "source": [
    "##  Teaching LLM workflow by using open source models and GPT4All\n",
    "GPT4all is a framework to source models and handle Jupyter workflow and have them  work within the confines of limited compute eg like a personal computer or a cloud based server like we use for instruction.  \n",
    "\n",
    "### Shared Filesystem \n",
    "\n",
    "In the setup where I was teaching I used this notebook to download models from Huggingface and I put them in a shared-readwrite folder, where the students could access them on Jupyterhub.  I was using a Jupyterhub for teaching that had a shared folder system.  \n",
    "\n",
    "Your use case may vary \n",
    "- shared read write\n",
    "- each student downloads own models\n",
    "- download models to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031df85-3599-4e95-a59d-f476cb36decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your python environment has gpt4all package installed.\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "except ImportError:\n",
    "    %pip install gpt4all\n",
    "    from gpt4all import GPT4All\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d330d",
   "metadata": {},
   "source": [
    "## Which model to download\n",
    "In the use case for teaching on a Juptyerhub with a CPU, I was looking for **small models**, \n",
    " - ~1bn parameters\n",
    " - quantized (Weights have 4 decimal places instead of 10 )\n",
    "\n",
    "\n",
    "(This info is as of the writing of this notebook in May/June 2025 and this info is changing rapidly) \n",
    "\n",
    "\n",
    "You can explore the world of models at :\n",
    "[Hugging Face Model List](https://huggingface.co/models)\n",
    "\n",
    "GPT4All is using a subset of these models - Here is the description from their [documentation](https://docs.gpt4all.io/gpt4all_desktop/models.html#explore-models):\n",
    "\n",
    "- Many LLMs are available at various sizes, quantizations, and licenses.\n",
    "\n",
    "- LLMs with more parameters tend to be better at coherently responding to instructions\n",
    "\n",
    "- LLMs with a smaller quantization (e.g. 4bit instead of 16bit) are much faster and less memory intensive, and tend to have slightly worse performance\n",
    "\n",
    "- Licenses vary in their terms for personal and commercial use\n",
    "\n",
    "\n",
    "\n",
    "Five that I picked to download are:\n",
    "- `DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf`\n",
    "- `Phi-3-mini-4k-instruct.Q4_0.gguf`\n",
    "- `Llama-3.2-1B-Instruct-Q4_0.gguf`\t\t \n",
    "- `qwen2-1_5b-instruct-q4_0.gguf`\n",
    "- `mistral-7b-instruct-v0.1.Q4_0.gguf`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273601bd",
   "metadata": {},
   "source": [
    "The simplest way to download a model is just to call for it in GPT4All and then it downloads it if you dont have it\n",
    "\n",
    "Don't worry if you get `llama_model_load: error loading model: error loading model vocabulary: unknown pre-tokenizer type: 'deepseek-r1-qwen'`  thats about access to GPUs which we don't have in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f784d27-ea55-4a2f-86e7-d08ff0e5f2f1",
   "metadata": {},
   "source": [
    "## Let's check out our local filesystem path and where we will download the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005c223-3f01-4478-92d6-3c2de6bb1a9d",
   "metadata": {},
   "source": [
    "### Approach 1 -  if a Shared Hub is being used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475f454-9d2a-45c2-9b98-11e3ee82304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only worked for SP 25 instuction on Berkeley Datahub\n",
    "#!ls /home/jovyan/_shared/econ148-readwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054e688-fa14-4990-aa56-be9aea92614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cal-ICOR workhop Hub?\n",
    "!ls /home/jovyan/shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595269ab-c7c0-4da5-8110-2907dfc36d86",
   "metadata": {},
   "source": [
    "### Approach 2 -  if a local machine is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fff6e-0a22-4d6f-ae3e-8c4c28d6d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is my local path to a directory called shared-rw\n",
    "!ls shared-rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f0889-5886-40ad-bd60-df4745851d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the full path ( this is on my laptop) \n",
    "!ls /Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238eb73-62e8-4cee-aeb3-34a81be11218",
   "metadata": {},
   "source": [
    "### Set the path where the models will download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ce40c-662d-4815-8339-0da6b45c59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for Shared Hub\n",
    "path = \"/home/jovyan/shared_readwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d6020-693b-4bef-8810-de0d21675bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for Local \n",
    "#path=\"/Users/ericvandusen/Documents/GitHub/SmallLM-SP25/shared-rw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d72df-194d-4209-b728-2ae92f5273f6",
   "metadata": {},
   "source": [
    "## Downloading the models\n",
    "\n",
    "In this cell, we define the `model` object — the interface through which the notebook will send all **prompts and conversations** to the local language model.\n",
    "\n",
    "The call uses the `GPT4All` class to load a quantized version of a model , a 2-billion-parameter instruction-tuned model from Google, optimized for efficient CPU inference.\n",
    "\n",
    "We specify:\n",
    "\n",
    "- **`model_name=\"XXXXXXX.gguf\"`** – identifies the specific quantized model file we’re using (in Q4\\_0 format, about ~1 GB in size).  \n",
    "- **`allow_download=True`** – allows GPT4All to automatically download the model if it isn’t found locally.  \n",
    "- **`model_path`** – the directory path where the model is stored or should be saved.  \n",
    "- **`verbose=True`** – prints detailed logs during model loading, useful for confirming that the model is correctly located and initialized.\n",
    "\n",
    "Once this cell runs successfully, the variable `model` will serve as our **connection point** for all local inference — the object we’ll send text prompts and chat messages to throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8a73d-e34f-4c52-bd7f-5e70f0b869d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e739378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"orca-mini-3b-gguf2-q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef346506-e0ee-4f13-be1f-a2090850f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"qwen2-1_5b-instruct-q4_0.gguf\",\n",
    "    allow_download=True,\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe9055-a741-4ec3-beb3-cb44f928163e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770b050-b19c-46dc-be78-ae76a13fab21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the \"model\" object to which this notebook's code will send conversations & prompts\n",
    "model = GPT4All(\n",
    "    model_name=\"Llama-3.2-1B-Instruct-Q4_0.gguf\",\n",
    "    model_path=path,\n",
    "    allow_download=True,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725bb95-4ed2-4a32-947a-279c751e3447",
   "metadata": {},
   "source": [
    "## Let's now check which models we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c9e6-936a-40be-b9a5-a213552b1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls \"{path}\" -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466141d-0062-4f9a-a26e-987f8f5f58c0",
   "metadata": {},
   "source": [
    "## Direct Download approach\n",
    "\n",
    "**Here is a command to do the download directly**\n",
    "\n",
    "- `!` - Jupyter magic that runs shell command in notebook\n",
    "- `wget` - Command-line tool for downloading files\n",
    "- `-c` - **Continue/Resume** - if download interrupted, picks up where it left off\n",
    "- `--progress=bar:force` - Shows download progress bar (% complete, speed, time remaining)\n",
    "- `-O [path]` - **Output** - specifies exact location and filename to save\n",
    "- `[URL]` - Direct download link to the GGUF model file on Hugging Face\n",
    "\n",
    "**The URL structure: of getting models from Huggingface**\n",
    "\n",
    "```\n",
    "https://huggingface.co/{organization}/{repo}/resolve/main/{filename}\n",
    "```\n",
    "\n",
    "- **Organization**: `allenai` (Allen Institute for AI)\n",
    "- **Repo**: `OLMo-2-0425-1B-GGUF`\n",
    "- **Branch**: `main`\n",
    "- **File**: `OLMo-2-0425-1B-Q4_K_M.gguf`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33bbebc-701c-415b-a86e-986d753b2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -c --progress=bar:force \\\n",
    "#  -O /home/jovyan/shared_readwrite/OLMo-2-0425-1B-Q4_K_M.gguf \\\n",
    "#  https://huggingface.co/allenai/OLMo-2-0425-1B-GGUF/resolve/main/OLMo-2-0425-1B-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856c4d5",
   "metadata": {},
   "source": [
    "## Bonus Searching for models from the GPT4All database \n",
    "\n",
    " - We can go to GPT4All database\n",
    " - Make that database into a pandas dataframe\n",
    " - Filter to pick nodels we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06680376-15c4-4cf3-aecb-c802e474352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a86bc-c0a5-461b-bb3a-5f1a33d89398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load JSON from the GPT4All models repository\n",
    "#Small curated list\n",
    "url = \"https://gpt4all.io/models/models3.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = requests.get(url).json()\n",
    "# Convert to DataFrame\n",
    "Models_df = pd.DataFrame(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceec60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c071ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns of the DataFrame\n",
    "Models_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b702f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensions of the DataFrame\n",
    "Models_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4772a-6315-4481-a669-0fe03afca03f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b30af0a-7874-43a9-97ca-353e32dd348d",
   "metadata": {},
   "source": [
    "**Looks like there are only 32 models in this dataset**\n",
    "\n",
    "`GPT4all` hasnt been keeping up with all that has been happening - but for students - this is a nice small world to look through!  There are 1000s of models on Huggingface - with different quantizations - and its completely overwhelming.  For pedagogical purposes the sandbox is helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be26b486-4494-4bf4-84eb-bfbe1989a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter models that require less than 4 GB of RAM\n",
    "# Convert 'ramrequired' to numeric and filter to ramrequired < 4\n",
    "Models_df[Models_df[\"ramrequired\"].astype(float) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6234c4d-eded-4111-8552-20a358c7cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jovyan/shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27cc56-f406-423f-adf2-c96e7a2f1461",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
