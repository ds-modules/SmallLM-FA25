{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside a Small Model: Understanding How LLMs Work\n",
    "## A Deep Dive with GPT4All\n",
    "\n",
    "This notebook explores **what happens inside a language model** when you give it a prompt. We'll visualize:\n",
    "- **Tokenization**: How text is broken into tokens\n",
    "- **Model Output**: What probabilities the model produces\n",
    "- **Decoding**: How tokens are selected to generate text\n",
    "\n",
    "This notebook uses the [GPT4All Python](https://docs.gpt4all.io/gpt4all_python/home.html) package and works with any local small model in `.gguf` format (OLMo, Mistral, Phi, Qwen, Llama, etc.).\n",
    "\n",
    "**No PyTorch required!** This runs entirely on CPU using GPT4All.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- GPT4All [Github Repo](https://github.com/nomic-ai/gpt4all)\n",
    "- GPT4All [Documentation](https://docs.gpt4all.io/gpt4all_python/home.html)\n",
    "- Behind the scenes: [llama.cpp](https://github.com/ggml-org/llama.cpp) backend\n",
    "\n",
    "### Attribution\n",
    "\n",
    "Notebook developed by Eric Van Dusen, building on work by Greg Merritt and the [ds-modules/ollama-demo](https://github.com/ds-modules/ollama-demo) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll:\n",
    "1. Install the GPT4All package if needed\n",
    "2. Set up the path to our model files\n",
    "3. Load a small model from disk\n",
    "\n",
    "This notebook assumes you have at least one `.gguf` model file downloaded (see `GPT4All_Download_gguf.ipynb` for download instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your python environment has gpt4all capability\n",
    "try:\n",
    "    from gpt4all import GPT4All\n",
    "except ImportError:\n",
    "    %pip install gpt4all\n",
    "    from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set the Model Path\n",
    "\n",
    "Choose the path based on your environment:\n",
    "- **Shared Hub** (like DataHub): `/home/jovyan/shared/`\n",
    "- **Local Jupyter**: Your custom path (e.g., `shared-rw` or full path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For shared hub deployment\n",
    "path = \"/home/jovyan/shared/\"\n",
    "\n",
    "# For local deployment (uncomment and modify as needed)\n",
    "# path = \"shared-rw\"\n",
    "# path = \"/Users/yourusername/path/to/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what models are available\n",
    "!ls \"{path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load a Small Model\n",
    "\n",
    "We'll load a small model (1-2B parameters, quantized to ~1GB). You can use any of these:\n",
    "- `qwen2-1_5b-instruct-q4_0.gguf` (Qwen 1.5B)\n",
    "- `Llama-3.2-1B-Instruct-Q4_0.gguf` (Llama 1B)\n",
    "- `Phi-3-mini-4k-instruct.Q4_0.gguf` (Phi-3 Mini)\n",
    "- `OLMo-2-0425-1B-Q4_K_M.gguf` (OLMo 1B)\n",
    "- Or any other `.gguf` model you've downloaded!\n",
    "\n",
    "<p style=\"background-color:#ffe6e6; color:#b30000; padding:6px; border-radius:6px;\">\n",
    "⚠️ Note: If you see a pink error box about CUDA, don't worry—that's normal. The model will run on CPU.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = GPT4All(\n",
    "    model_name=\"qwen2-1_5b-instruct-q4_0.gguf\",  # Change this to use a different model\n",
    "    model_path=path,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Tokenization\n",
    "\n",
    "Language models don't work with raw text—they work with **tokens**. A token is a chunk of text (could be a word, part of a word, or punctuation).\n",
    "\n",
    "Let's explore how text gets broken into tokens.\n",
    "\n",
    "### What is a Token?\n",
    "\n",
    "- Common words are usually single tokens: `\"hello\"` → `[hello]`\n",
    "- Uncommon words may be split: `\"tokenization\"` → `[token, ization]`\n",
    "- Spaces and punctuation count too: `\"Hello, world!\"` → `[Hello, \",\", \" world\", \"!\"]`\n",
    "\n",
    "Different models use different tokenizers, so the same text may tokenize differently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize tokenization\n",
    "# GPT4All doesn't expose the tokenizer directly, but we can demonstrate the concept\n",
    "\n",
    "def visualize_text_chunks(text):\n",
    "    \"\"\"\n",
    "    Simple demonstration of how text might be chunked.\n",
    "    Note: This is illustrative - actual tokenization varies by model.\n",
    "    \"\"\"\n",
    "    print(f\"Original text: '{text}'\")\n",
    "    print(f\"Length in characters: {len(text)}\")\n",
    "    print(\"\\nRough token visualization (simplified):\")\n",
    "    \n",
    "    # Approximate token count (GPT models average ~4 chars per token)\n",
    "    approx_tokens = len(text) / 4\n",
    "    print(f\"Approximate token count: {int(approx_tokens)}\")\n",
    "    \n",
    "    # Show word-level breakdown as a simple approximation\n",
    "    words = text.split()\n",
    "    print(f\"\\nWord breakdown ({len(words)} words):\")\n",
    "    for i, word in enumerate(words, 1):\n",
    "        print(f\"  {i}. '{word}'\")\n",
    "\n",
    "# Try different examples\n",
    "examples = [\n",
    "    \"Hello, world!\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Antidisestablishmentarianism\",\n",
    "    \"AI is transforming society.\"\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    visualize_text_chunks(example)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Token Limits and Max Tokens\n",
    "\n",
    "Every model has a **context window** (maximum number of tokens it can process) and you can set a **max_tokens** parameter to limit output length.\n",
    "\n",
    "- **Context window**: Total tokens (prompt + response). Often 2048-4096 for small models.\n",
    "- **max_tokens**: Maximum tokens in the *response only*.\n",
    "\n",
    "If prompt + response exceeds the context window, the model can't process it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate text with different token limits\n",
    "prompt = \"Explain what a language model is in simple terms.\"\n",
    "\n",
    "print(\"Short response (20 tokens):\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=20)\n",
    "    print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Longer response (100 tokens):\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=100)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Model Output: Probabilities\n",
    "\n",
    "When a language model processes text, it doesn't directly output words. Instead, it outputs **probabilities** for every possible next token.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Input**: The model receives tokens (e.g., \"The cat sat\")\n",
    "2. **Processing**: Neural network computes probabilities for what comes next\n",
    "3. **Output**: A probability distribution over all ~50,000 possible tokens\n",
    "4. **Selection**: One token is chosen based on these probabilities\n",
    "5. **Repeat**: That token is added, and the process repeats\n",
    "\n",
    "Let's visualize this concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate what probability distributions might look like\n",
    "import random\n",
    "\n",
    "def simulate_next_token_probabilities(prompt, top_n=10):\n",
    "    \"\"\"\n",
    "    Simulates what next-token probabilities might look like.\n",
    "    Note: This is illustrative - actual model probabilities are computed internally.\n",
    "    \"\"\"\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"\\nTop {top_n} probable next tokens (simulated):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Common words that might follow different prompts\n",
    "    if \"cat\" in prompt.lower():\n",
    "        candidates = [(\"sat\", 0.35), (\"jumped\", 0.20), (\"ran\", 0.15), \n",
    "                     (\"meowed\", 0.12), (\"slept\", 0.10), (\"walked\", 0.05),\n",
    "                     (\"played\", 0.02), (\"ate\", 0.01)]\n",
    "    elif \"the\" in prompt.lower():\n",
    "        candidates = [(\"quick\", 0.15), (\"cat\", 0.14), (\"dog\", 0.13),\n",
    "                     (\"answer\", 0.12), (\"question\", 0.11), (\"world\", 0.10),\n",
    "                     (\"sky\", 0.09), (\"earth\", 0.08), (\"sun\", 0.08)]\n",
    "    else:\n",
    "        candidates = [(\"is\", 0.25), (\"the\", 0.20), (\"and\", 0.15),\n",
    "                     (\"to\", 0.12), (\"of\", 0.10), (\"in\", 0.08),\n",
    "                     (\"that\", 0.05), (\"it\", 0.05)]\n",
    "    \n",
    "    for i, (token, prob) in enumerate(candidates[:top_n], 1):\n",
    "        bar = \"█\" * int(prob * 100)\n",
    "        print(f\"{i:2d}. '{token:12s}' {prob:5.1%} {bar}\")\n",
    "    \n",
    "    print(\"\\nThe model chooses one token based on these probabilities.\")\n",
    "    print(\"Higher probability = more likely to be chosen\")\n",
    "\n",
    "# Try different prompts\n",
    "simulate_next_token_probabilities(\"The cat\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "simulate_next_token_probabilities(\"The\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Why Probabilities Matter\n",
    "\n",
    "The model always produces probabilities, but **how we select from them** determines the output quality:\n",
    "\n",
    "- **Greedy decoding**: Always pick the highest probability (deterministic)\n",
    "- **Sampling**: Randomly sample based on probabilities (varied)\n",
    "- **Temperature**: Controls randomness in sampling\n",
    "\n",
    "We'll explore these next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Decoding: How Tokens Are Selected\n",
    "\n",
    "**Decoding** is the process of choosing which token to use next based on the model's probability distribution.\n",
    "\n",
    "### 4.1 Temperature: Controlling Randomness\n",
    "\n",
    "**Temperature** affects how \"random\" or \"conservative\" the model's choices are:\n",
    "\n",
    "- **Temperature = 0**: Always pick the highest probability token (\"greedy\" decoding)\n",
    "  - ✅ Deterministic—same input always gives same output\n",
    "  - ✅ Factual and conservative\n",
    "  - ❌ Repetitive and boring\n",
    "\n",
    "- **Temperature = 0.5**: Slightly random, but still favors likely tokens\n",
    "  - ✅ Some variation while staying reasonable\n",
    "  - Good for most applications\n",
    "\n",
    "- **Temperature = 1.0**: Full randomness based on probabilities\n",
    "  - ✅ Creative and diverse\n",
    "  - ❌ Can be less coherent\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same prompt, different temperatures\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "num_responses = 3\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPERATURE = 0 (Greedy/Deterministic)\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_responses):\n",
    "    with model.chat_session():\n",
    "        response = model.generate(prompt=prompt, max_tokens=30, temp=0.0)\n",
    "        print(f\"\\nResponse {i+1}: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEMPERATURE = 0.7 (Balanced)\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_responses):\n",
    "    with model.chat_session():\n",
    "        response = model.generate(prompt=prompt, max_tokens=30, temp=0.7)\n",
    "        print(f\"\\nResponse {i+1}: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEMPERATURE = 1.0 (Creative/Random)\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(num_responses):\n",
    "    with model.chat_session():\n",
    "        response = model.generate(prompt=prompt, max_tokens=30, temp=1.0)\n",
    "        print(f\"\\nResponse {i+1}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Top-k and Top-p Sampling\n",
    "\n",
    "Besides temperature, we can control decoding with:\n",
    "\n",
    "**Top-k sampling**: Only consider the top k most likely tokens\n",
    "- `top_k=1`: Greedy (always pick the best)\n",
    "- `top_k=10`: Choose randomly from the 10 most likely tokens\n",
    "- `top_k=50`: Choose from top 50 (more variety)\n",
    "\n",
    "**Top-p sampling** (nucleus sampling): Choose from tokens whose cumulative probability is ≤ p\n",
    "- `top_p=0.1`: Very conservative (only very likely tokens)\n",
    "- `top_p=0.9`: More diverse (include less likely tokens)\n",
    "- `top_p=1.0`: Consider all tokens\n",
    "\n",
    "Let's experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with top_k\n",
    "prompt = \"Once upon a time, in a land far away,\"\n",
    "\n",
    "print(\"Top-k = 1 (Greedy):\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=40, temp=1.0, top_k=1)\n",
    "    print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"Top-k = 40 (More variety):\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=40, temp=1.0, top_k=40)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenting with top_p\n",
    "prompt = \"The meaning of life is\"\n",
    "\n",
    "print(\"Top-p = 0.1 (Conservative):\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=40, temp=1.0, top_p=0.1)\n",
    "    print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"Top-p = 0.95 (More diverse):\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=40, temp=1.0, top_p=0.95)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting It All Together: Generation Step-by-Step\n",
    "\n",
    "Let's trace through what happens during text generation:\n",
    "\n",
    "1. **Tokenization**: Your prompt is broken into tokens\n",
    "2. **Model Processing**: Tokens go through the neural network\n",
    "3. **Probability Computation**: Model outputs probabilities for next token\n",
    "4. **Sampling**: A token is selected using temperature/top-k/top-p\n",
    "5. **Append**: Selected token is added to the sequence\n",
    "6. **Repeat**: Steps 2-5 repeat until max_tokens or stop condition\n",
    "\n",
    "### 5.1 Visualizing Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_generation_steps(prompt, max_tokens=5):\n",
    "    \"\"\"\n",
    "    Simulates and visualizes the step-by-step generation process.\n",
    "    Note: This is a conceptual demonstration - actual generation happens internally.\n",
    "    \"\"\"\n",
    "    print(\"STEP-BY-STEP GENERATION SIMULATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Initial prompt: '{prompt}'\")\n",
    "    print(f\"Max tokens to generate: {max_tokens}\")\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    current_text = prompt\n",
    "    \n",
    "    # Simulate token-by-token generation\n",
    "    sample_tokens = [\"beautiful\", \"mysterious\", \"ancient\", \"forgotten\", \"kingdom\"]\n",
    "    \n",
    "    for step in range(max_tokens):\n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(f\"  Current text: '{current_text}'\")\n",
    "        print(f\"  → Model computes probabilities for next token...\")\n",
    "        \n",
    "        # Simulate token selection\n",
    "        next_token = sample_tokens[step] if step < len(sample_tokens) else \".\"\n",
    "        print(f\"  → Selected token: '{next_token}'\")\n",
    "        \n",
    "        current_text += \" \" + next_token\n",
    "        print(f\"  Updated text: '{current_text}'\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Final generated text: '{current_text}'\")\n",
    "\n",
    "simulate_generation_steps(\"Once upon a time, there was a\", max_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Streaming Generation\n",
    "\n",
    "In real applications, we often want to see tokens as they're generated (like ChatGPT). This is called **streaming**.\n",
    "\n",
    "GPT4All supports streaming, which lets us see each token as it's produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming example - watch tokens appear one at a time\n",
    "import sys\n",
    "\n",
    "prompt = \"Write a haiku about small language models:\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "with model.chat_session():\n",
    "    tokens = model.generate(prompt=prompt, max_tokens=50, temp=0.7, streaming=True)\n",
    "    \n",
    "    for token in tokens:\n",
    "        print(token, end='', flush=True)\n",
    "        \n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"\\nGeneration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Controlling Generation: Repetition and Penalties\n",
    "\n",
    "Sometimes models repeat themselves. We can use **repetition penalty** to discourage this.\n",
    "\n",
    "- `repeat_penalty=1.0`: No penalty (default)\n",
    "- `repeat_penalty=1.2`: Slightly discourage repetition\n",
    "- `repeat_penalty=1.5`: Strongly discourage repetition\n",
    "- `repeat_last_n`: How many recent tokens to check for repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reducing repetition\n",
    "prompt = \"The cat sat on the mat. The cat\"\n",
    "\n",
    "print(\"Without repetition penalty:\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=30, temp=0.8, repeat_penalty=1.0)\n",
    "    print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"With repetition penalty (1.3):\")\n",
    "with model.chat_session():\n",
    "    response = model.generate(prompt=prompt, max_tokens=30, temp=0.8, repeat_penalty=1.3)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Different Models\n",
    "\n",
    "Different models (OLMo, Mistral, Phi, Qwen, Llama) behave differently even with the same prompt and parameters.\n",
    "\n",
    "This is because they:\n",
    "- Were trained on different data\n",
    "- Have different architectures\n",
    "- Use different tokenizers\n",
    "- Have different sizes\n",
    "\n",
    "### 7.1 Switching Models\n",
    "\n",
    "To try a different model, just change the `model_name` when loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a different model\n",
    "# Uncomment to try (make sure you have the model downloaded first!)\n",
    "\n",
    "# model_olmo = GPT4All(\n",
    "#     model_name=\"OLMo-2-0425-1B-Q4_K_M.gguf\",\n",
    "#     model_path=path,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# model_llama = GPT4All(\n",
    "#     model_name=\"Llama-3.2-1B-Instruct-Q4_0.gguf\",\n",
    "#     model_path=path,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# model_phi = GPT4All(\n",
    "#     model_name=\"Phi-3-mini-4k-instruct.Q4_0.gguf\",\n",
    "#     model_path=path,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "print(\"To compare models, load multiple models and run the same prompt on each!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Tokenization**\n",
    "   - Text is broken into tokens (words, subwords, characters)\n",
    "   - Models have token limits (context window)\n",
    "   - Different models use different tokenizers\n",
    "\n",
    "2. **Model Output**\n",
    "   - Models output probability distributions, not words\n",
    "   - Every token has a probability of being next\n",
    "   - Generation selects tokens based on these probabilities\n",
    "\n",
    "3. **Decoding Strategies**\n",
    "   - **Temperature**: Controls randomness (0 = deterministic, 1+ = creative)\n",
    "   - **Top-k**: Limit to k most likely tokens\n",
    "   - **Top-p**: Limit to tokens with cumulative probability ≤ p\n",
    "   - **Repetition penalty**: Discourage repeating tokens\n",
    "\n",
    "4. **Generation Process**\n",
    "   - One token at a time\n",
    "   - Each token depends on all previous tokens\n",
    "   - Can be streamed for real-time display\n",
    "\n",
    "### Practical Applications:\n",
    "\n",
    "- **Factual answers**: Use low temperature (0-0.3), greedy decoding\n",
    "- **Creative writing**: Use higher temperature (0.7-1.0), top-p sampling\n",
    "- **Code generation**: Use low temperature, repetition penalty\n",
    "- **Brainstorming**: Use high temperature, high top-k\n",
    "\n",
    "### Remember:\n",
    "\n",
    "- Models don't \"understand\"—they predict probable next tokens\n",
    "- Higher randomness ≠ better quality (just more varied)\n",
    "- Small models (~1B params) work well for many tasks\n",
    "- No internet or GPU needed with GPT4All!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiments to Try\n",
    "\n",
    "Now that you understand how models work internally, try these experiments:\n",
    "\n",
    "### Experiment 1: Temperature Extremes\n",
    "Try temperature = 0, 0.5, 1.0, and 2.0 on the same prompt. What happens at 2.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment here\n",
    "prompt = \"The most important invention in history was\"\n",
    "\n",
    "# Try different temperatures and observe the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Token Limits\n",
    "Try max_tokens = 5, 20, 100, 200. How does response quality change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment here\n",
    "prompt = \"Explain quantum computing\"\n",
    "\n",
    "# Try different max_tokens values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Comparing Models\n",
    "Load different models (OLMo, Llama, Phi, Qwen) and compare their responses to the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment here\n",
    "prompt = \"What makes a good teacher?\"\n",
    "\n",
    "# Load and compare different models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: System Prompts\n",
    "Use a system prompt to give the model a \"personality\" or role. How does this affect outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiment here\n",
    "system_prompt = \"You are a helpful but very concise assistant who answers in haiku.\"\n",
    "user_prompt = \"What is machine learning?\"\n",
    "\n",
    "# Try different system prompts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Further Reading\n",
    "\n",
    "Want to learn more?\n",
    "\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual explanation of how transformers work\n",
    "- [GPT4All Documentation](https://docs.gpt4all.io/) - Full API reference\n",
    "- [Hugging Face Model Hub](https://huggingface.co/models) - Browse thousands of models\n",
    "- [Understanding GGUF format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) - How quantized models work\n",
    "\n",
    "### Related Notebooks in This Repo:\n",
    "\n",
    "- `GPT4All_SmallLM_Demo.ipynb` - Basic usage and chat sessions\n",
    "- `GPT4All_Download_gguf.ipynb` - How to download models\n",
    "- `Gradio_Chatbot_GPT4All.ipynb` - Build a chatbot UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
